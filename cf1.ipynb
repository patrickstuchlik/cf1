#%% Change working directory from the workspace root to the ipynb file location. Turn this addition off with the DataScience.changeDirOnImportExport setting
# ms-python.python added
import os
try:
	os.chdir(os.path.join(os.getcwd(), '..\\..\..\Downloads'))
	print(os.getcwd())
except:
	pass
#%% [markdown]
# Briefly, our goal here is to model the association of life expectancy, as measured by the CDC, with several features of the built environment in each census tract in the city of Saint Louis, Missouri. We begin by reading in the data.

#%%
url = 'https://github.com/patrickstuchlik/cap1/raw/master/elastic2.csv'
import pandas as pd
df = pd.read_csv(url)
#df.head()
df2 = df.set_index('GEOID')
df2.head()

#%% [markdown]
# A quick word about each of our measured variables:
# 
# 
# 
# 
# 
# *   **'GEOID'** : the 11-digit FIPS code for each census tract
# *   **'land_perc'** : the percent of each census tract that GIS calculated to be land (i.e. not water)
# *   **'food1'** : a USDA calculated percentage of the total census tract with [low access to healthy food](https://www.ers.usda.gov/data-products/food-access-research-atlas/documentation/) within 1 mile
# * **'food2'** : similarly, the percentage of the low-income population in the census tract with low access to healthy food within 1 mile
# * **'life'** : [life expectancy](https://www.cdc.gov/nchs/nvss/usaleep/usaleep.html), in years, at birth for the census tract
# * **'mw'** : number of [milkweed gardens](https://www.stlouis-mo.gov/monarchs/) in each census tract, planted to encourage the monarch butterfly population
# * **'cg'** : number of [community gardens](http://www.gatewaygreening.org/resources/map-of-gardens/) in each census tract
# * **'metrolink'** : number of [Metrolink](https://www.metrostlouis.org/developer-resources/) (light rail) stations in each census tract
# * **'tract_area'** : area (units unclear due to GIS projection issues) of each census tract
# * **'bike_len'** : total length of [bike lanes](https://www.stlouis-mo.gov/government/departments/street/streets-sidewalks-traffic/bicycling/bike-routes-and-maps.cfm) for each census tract
# * **'bike_cnt'**: total segments of bike lanes for each census tract
# * **'park1'** : total area of [parks](https://www.stlouis-mo.gov/data/parks.cfm) in each census tract (for similar reasons, units unclear)
# * **'park1ratio'** : ratio of park1 / tract_area => the ratio of a census tract that is public parks
# * **'park2' , 'park2ratio'** : a different (and inferior) polygon intersection tool was used in GIS to compute these park areas, in the same schema as park1 and park1ratio
# * **'walk'** : National Walkability Index from the [EPA](https://www.epa.gov/smartgrowth/smart-location-mapping#walkability) Note that these scores are provided for census block groups, so these have been averaged over tracts here
# * **'unins'**: percentage of the adult non-institutionalized population without health insurance in each census tract, from [Census American FactFinder](https://factfinder.census.gov/faces/nav/jsf/pages/index.xhtml)
# 
# 
# 

#%%
X = df2[['food1','mw','cg','metrolink','bike_len','park1','walk','unins']]
y = df2['life']

#%% [markdown]
# A brief inspection of the distribution of life expectancy over the 106 census tracts in the city of St. Louis:

#%%
import matplotlib.pyplot as plt

n, bins, patches = plt.hist(x=df2['life'], bins='auto', color='red', alpha=0.7, rwidth=0.85)

#%% [markdown]
# For the following analyses, we will focus on ~ eight features of the built environment: healthy food access, milkweed gardens, community gardens, Metrolink stations, bike lanes, park area, Walkability, and percent uninsured, a proxy for socioeconomic status relevant to this health outcome measure. 
# 
# The below cell will give you a scatter matrix of the relationships of all variables with each other, but it's not too useful.

#%%
pd.plotting.scatter_matrix(X)

#%% [markdown]
# The below cell will provide the scatter plots of each of these eight features with life expectancy.

#%%
cols = [col for col in X.columns]

for col in cols:
  df2.plot.scatter(x=col,y='life')

#%% [markdown]
# Now that we have our X and y data, we will split the data into test and training sets for machine learning algorithms.

#%%
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)

#%% [markdown]
# We also imported a scaler to use in preprocessing in the event that our eventual model performs better with scaled data.

#%%
from sklearn.preprocessing import StandardScaler
scal = StandardScaler()
X_train_scaled = scal.fit_transform(X_train)
X_test_scaled = scal.fit_transform(X_test)

#%% [markdown]
# The next step was to test several models to have an idea which to pursue next. I tested a basic linear regression, along with lasso, elastic-net, and ridge linear models. I also tested a support vector machine regression (with radial basis function), as well as three ensemble models: random forest, gradient boosting, and extra trees regressions. The idea here is to collect the MSE for each basic model on the training data, and then choose the model with the best score performance.

#%%
#do the model checking
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import Lasso
from sklearn.linear_model import Ridge
from sklearn.linear_model import ElasticNet
from sklearn import svm
from sklearn import ensemble
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

num_folds = 5
scoring = "neg_mean_squared_error"
#scoring = 'r2'

models = []
models.append(('LR', LinearRegression()))
models.append(('LASSO', Lasso()))
models.append(('RIDGE', Ridge() ))
models.append(('EN', ElasticNet()))
models.append(('SVR', svm.SVR(kernel='rbf',gamma='auto')))
#models.append(('LINSVR', svm.LinearSVR()))
models.append(('RFR', ensemble.RandomForestRegressor(n_estimators=100)))
models.append(('GBR', ensemble.GradientBoostingRegressor(n_estimators=100)))
models.append(('ETR', ensemble.ExtraTreesRegressor(n_estimators=100)))

results = []
names = []
for name, model in models:
    kfold = KFold(n_splits=num_folds, random_state=0)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold,    scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(),   cv_results.std())
    print(msg)

#%% [markdown]
# It appears the gradient boosting regression performs much better than the others (lower is better), but below I've visualized every model's MSE.

#%%
from matplotlib import pyplot

fig = pyplot.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
pyplot.boxplot(results)
ax.set_xticklabels(names)
pyplot.show()

#%% [markdown]
# So I decided to pursue the gradient boosting regression model. The next step is to consider feature decomposition (since I have a small dataset for eight features), and then to tune the hyperparameters. Therefore, two of the following pipelines include PCA (an easy option for continuous features). All of these pipelines are fed into cross-validation grid searches.
# 
# N.B. I made a mistake earlier in my analysis that suggested SVM may have had the best MSE. I've kept my code in this notebook to demonstrate another example of a grid search pipeline.

#%%
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA


#%%
est1 = [('reduce_dim', PCA()), ('svr', svm.SVR())]
pipe1 = Pipeline(steps=est1)
est2 = [('reduce_dim', PCA()), ('gbr', ensemble.GradientBoostingRegressor())]
pipe2 = Pipeline(steps=est2)
pipe3 = Pipeline(steps = [('gbr', ensemble.GradientBoostingRegressor())])


#%%
param_grid1 = dict(reduce_dim__n_components=[2,3,4,5], svr__C=[0.01, 0.1, 10, 100,1000], svr__gamma = [0.1,0.01])
param_grid2 = dict(reduce_dim__n_components=[2,3,4], gbr__n_estimators=[5,10,40,50,60,100], gbr__learning_rate = [0.1,0.05,0.02,0.01,0.005],gbr__max_depth=[1,2,3,4],gbr__min_samples_leaf=[1,2])
param_grid3 = dict(gbr__n_estimators=[2,5,10,25,50,100,200,500], gbr__learning_rate = [0.1,0.05,0.02,0.01,0.005],gbr__max_depth=[1,2,3,4],gbr__min_samples_split=[2,3],gbr__min_samples_leaf=[1,2])


#%%
from sklearn.model_selection import GridSearchCV
gs1 = GridSearchCV(pipe1, param_grid=param_grid1, cv=5)
gs2 = GridSearchCV(pipe2, param_grid=param_grid2, cv=5)
gs3 = GridSearchCV(pipe3, param_grid=param_grid3, cv=5)


#%%
gs3.fit(X_train, y_train)


#%%
gs3.best_params_


#%%
gs3.score(X,y)

#%% [markdown]
# After comparing **gs2** (pipeline beginning with PCA) with **gs3** (no PCA),  the higher coefficient of determination occurred without PCA. This may be because gradient boosting regression already considers the number of features, somewhat obviating the need for PCA. 

#%%
gs3.predict(X)

#%% [markdown]
# We can now plot the distribution of predicted life expectancies over the measured life expectancies.

#%%
import matplotlib.pyplot as plt

n, bins, patches = plt.hist(x=df2['life'], bins='auto', color='red', alpha=0.7, rwidth=0.85)
#n, bins, patches = plt.hist(x=clf.predict(X), bins='auto', color='blue', alpha=0.7, rwidth=0.85)
#n, bins, patches = plt.hist(x=ridg.predict(X), bins='auto', color='green', alpha=0.7, rwidth=0.85)
n, bins, patches = plt.hist(x=gs3.predict(X), bins='auto', color='blue', alpha=0.7, rwidth=0.85)

#%% [markdown]
# This is consistent with R^2 = 0.25. Not too impressive, but we can have some confidence that the model is not overfit or underfit.
# 
# For the fun of it, I also grossly overfit a model.

#%%
gbr = ensemble.GradientBoostingRegressor(n_estimators=100, learning_rate=0.05,max_depth=4)
gbr.fit(X_train,y_train)
gbr.score(X,y)

#%% [markdown]
# Of course, this model fits the data very well.

#%%
n, bins, patches = plt.hist(x=df2['life'], bins='auto', color='red', alpha=0.7, rwidth=0.85)
#n, bins, patches = plt.hist(x=clf.predict(X), bins='auto', color='blue', alpha=0.7, rwidth=0.85)
#n, bins, patches = plt.hist(x=ridg.predict(X), bins='auto', color='green', alpha=0.7, rwidth=0.85)
n, bins, patches = plt.hist(x=gbr.predict(X), bins='auto', color='blue', alpha=0.7, rwidth=0.85)


#%%
outp = gs3.predict(X)


#%%
df3 = df2
df3['outp'] = outp


#%%
df3.head()


#%%
import geopandas as gpd
# from urllib.request import urlopen
# from zipfile import ZipFile
# import zipfile
# import io
# import shapefile
# from shapely.geometry import shape  
# import osr
import pandas as pd
import requests
import matplotlib.pyplot as plt
import json

url2 = 'https://raw.githubusercontent.com/patrickstuchlik/shape1/master/stl.json'
r = requests.get(url2)

gd2 = json.loads(r.content)

gd3 = json.JSONEncoder().encode(gd2)

gdf = gpd.read_file(gd3).set_index('id')


#%%
from bokeh.io import output_notebook, show, output_file, curdoc
from bokeh.plotting import figure
from bokeh.models import GeoJSONDataSource, LinearColorMapper, ColorBar, HoverTool, Select, AdaptiveTicker, ColumnDataSource
from bokeh.palettes import brewer
import bokeh
from bokeh import palettes
from bokeh.layouts import row, column

def get_dataset(src,bg):
    return GeoJSONDataSource(geojson = gd3)

#Input GeoJSON source that contains features for plotting.
#geosource = GeoJSONDataSource(geojson = json_data)
def make_plot(source,bgvar):
    #Define a sequential multi-hue color palette.
    palette = bokeh.palettes.Plasma[7]
    #Reverse color order so that dark blue is highest obesity.
    palette = palette[::-1]
    #Instantiate LinearColorMapper that linearly maps numbers in a range, into a sequence of colors.
    color_mapper = LinearColorMapper(palette = palette, low= int(gdf[bgvar].min())    , high=int(gdf[bgvar].max()))
    #Define custom tick labels for color bar.
    #tick_labels = {'<62.0': '<62.0', '62-65': '62-65', '65-68':'65-68', '68-71':'68-71', '71-74':'71-74', '74-77':'74-77', '>77.0':'>77.0'}
    #Add hover tool
    hover = HoverTool(tooltips = [ ('Life Expectancy','@outp_life'),('Predicted Life Expectancy','@outp_outp'),(str(bgvar),('@'+str(bgvar)))])
    #Create color bar.
    ticker = AdaptiveTicker()
    #ticker.desired_num_ticks=10
    color_bar = ColorBar(color_mapper=color_mapper, label_standoff=8,width = 20, height = 500,
    border_line_color=None,location = (0,0), orientation = 'vertical',ticker=ticker)
    #Create figure object.
    p = figure(title = 'Life Expectancy', plot_height = 600 , plot_width = 450, toolbar_location = None, tools=[hover])
    p.xgrid.grid_line_color = None
    p.ygrid.grid_line_color = None
    #Add patch renderer to figure. 
    p.patches('xs','ys', source = source,fill_color = {'field' :str(bgvar), 'transform' : color_mapper}, line_color = 'black', line_width = 0.25, fill_alpha = 1)
    #Specify figure layout.
    p.add_layout(color_bar, 'right')
    return p

def update_plot(attrname, old, new):
       bef1 = bef[bef_select.value]

       src = get_dataset(gdf,bef_select.value)
       source.data.update(src.data)              
       
       
       


    

bef = {'Food Access' : 'outp_food1',
       'Milkweed' : 'outp_mw',
       'Community Gardens' : 'outp_cg',
       'Metrolink' : 'outp_metro',
       'Bike Lanes' : 'outp_bike_',
       'Public Parks' : 'outp_park1',
       'Walkability' : 'outp_walk',
       'Uninsured' : 'outp_unins'
}


#Display figure.
bef1 = 'Uninsured'

bef_select = Select(value=bef1, title='Built Environment Feature', options=sorted(bef.keys()))

source = get_dataset(gdf,bef1)
plot1 = make_plot(source,bef[bef1])

bef_select.on_change('value', update_plot)

controls = bef_select

# curdoc().add_root(row(plot1,controls))
# curdoc().title = "Life Expectancy"

output_notebook()
show(row(plot1,controls))


#%%



